{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Programming and Data Analysis\n",
    "\n",
    "> Final\n",
    "\n",
    "Kuo, Yao-Jen <yaojenkuo@ntu.edu.tw> from [DATAINPOINT](https://www.datainpoint.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Instructions\n",
    "\n",
    "- The assignment will be disconnected if idling over 10 minutes, we can reactivate a new session by clicking the assignment link again.\n",
    "- We've imported necessary modules at the top of each assignment.\n",
    "- We've put necessary files(if any) in the working directory.\n",
    "- We've defined the names of functions/inputs/parameters for you.\n",
    "- Write down your solution between the comments `### BEGIN SOLUTION` and `### END SOLUTION`.\n",
    "- It is NECESSARY to `return` the answer, tests will fail by just printing out the answer.\n",
    "- It is known that errors like `SyntaxError` or `IndentationError` breaks our `test_runner.py` and results in a zero point grade. It is highly recommended testing your solution by calling functions/methods in notebook or running tests before submission.\n",
    "- Running tests to see if your solutions are right:\n",
    "    - File -> Save Notebook to save `final.ipynb`.\n",
    "    - File -> New -> Terminal to open a Terminal.\n",
    "    - Use command `python test_runner.py` to run test.\n",
    "- When you are ready to submit, click File -> Export Notebook As -> Executable Script.\n",
    "- Rename the exported Python script with your student ID(e.g. `b01234567.py`) and upload to the Assignment session on NTU COOL/NTNU Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Define a function `find_the_best_two_teams()` which returns the best 2 teams in the western conference and the eastern conference, respectively. Look into the `[\"league\"][\"standard\"]` and `confRank == \"1\"` attributes in `standings_all.json` and `teams.json` for answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_best_two_teams() -> tuple:\n",
    "    \"\"\"\n",
    "    >>> best_team_in_west, best_team_in_east = find_the_best_two_teams()\n",
    "    >>> type(best_team_in_west)\n",
    "    dict\n",
    "    >>> type(best_team_in_east)\n",
    "    dict\n",
    "    >>> best_team_in_west[\"teamId\"]\n",
    "    '1610612744'\n",
    "    >>> best_team_in_east[\"teamId\"]\n",
    "    '1610612741'\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    opt = []\n",
    "    best_team_in_west = {}\n",
    "    best_team_in_east = {}\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for i in package['league']['standard']['teams']:\n",
    "            if i['confRank'] == '1':\n",
    "                opt.append(i['teamId'])\n",
    "    f.close()\n",
    "    with open(file=\"teams.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for j in range(len(opt)):\n",
    "            for i in package['league']['standard']:\n",
    "                if i['teamId'] == opt[j]:\n",
    "                    if i['confName'] == 'East':\n",
    "                        best_team_in_east['teamId'] = opt[j]\n",
    "                        best_team_in_east['nickname'] = i['nickname']\n",
    "                        best_team_in_east['city'] = i['city']\n",
    "                    else:\n",
    "                        best_team_in_west['teamId'] = opt[j]\n",
    "                        best_team_in_west['nickname'] = i['nickname']\n",
    "                        best_team_in_west['city'] = i['city']\n",
    "    return best_team_in_west, best_team_in_east\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Define a function `find_the_worst_two_teams()` which returns the worst 2 teams in the western conference and the eastern conference, respectively. Look into the `[\"league\"][\"standard\"]` and `confRank == \"15\"` attributes in `standings_all.json` and `teams.json` for answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_worst_two_teams() -> tuple:\n",
    "    \"\"\"\n",
    "    >>> worst_team_in_west, worst_team_in_east = find_the_worst_two_teams()\n",
    "    >>> type(worst_team_in_west)\n",
    "    dict\n",
    "    >>> type(worst_team_in_east)\n",
    "    dict\n",
    "    >>> worst_team_in_west[\"teamId\"]\n",
    "    '1610612745'\n",
    "    >>> worst_team_in_east[\"teamId\"]\n",
    "    '1610612753'\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    opt = []\n",
    "    worst_team_in_west = {}\n",
    "    worst_team_in_east = {}\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for i in package['league']['standard']['teams']:\n",
    "            if i['confRank'] == '15':\n",
    "                opt.append(i['teamId'])\n",
    "    f.close()\n",
    "    with open(file=\"teams.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for j in range(len(opt)):\n",
    "            for i in package['league']['standard']:\n",
    "                if i['teamId'] == opt[j]:\n",
    "                    if i['confName'] == 'East':\n",
    "                        worst_team_in_east['teamId'] = opt[j]\n",
    "                        worst_team_in_east['nickname'] = i['nickname']\n",
    "                        worst_team_in_east['city'] = i['city']\n",
    "                    else:\n",
    "                        worst_team_in_west['teamId'] = opt[j]\n",
    "                        worst_team_in_west['nickname'] = i['nickname']\n",
    "                        worst_team_in_west['city'] = i['city']\n",
    "    return worst_team_in_west, worst_team_in_east\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Define a function named `extract_current_standings` which extracts the team standings given `standings_all.json`. Look into the `[\"league\"][\"standard\"]` in `standings_all.json` for answer. Make sure the number of wins/losses are able to perform numeric operations.\n",
    "\n",
    "```\n",
    "        teamId  homeWin  homeLoss  awayWin  awayLoss  win  loss\n",
    "0   1610612744       18         3       12         6   30     9\n",
    "1   1610612756       17         5       13         4   30     9\n",
    "2   1610612741       14         4       12         7   26    11\n",
    "3   1610612762       14         7       14         6   28    13\n",
    "4   1610612763       14         8       14         6   28    14\n",
    "5   1610612751       11        10       14         4   25    14\n",
    "6   1610612748       12         4       13        11   25    15\n",
    "7   1610612749       13         7       13        10   26    17\n",
    "8   1610612755        8         8       15         8   23    16\n",
    "9   1610612739       11         9       12         9   23    18\n",
    "10  1610612742       11         8       11        10   22    18\n",
    "11  1610612761       12        10        8         7   20    17\n",
    "12  1610612766       12         5       10        14   22    19\n",
    "13  1610612743        9         7       11        11   20    18\n",
    "14  1610612747       14        11        7         9   21    20\n",
    "15  1610612750       11        10        9        10   20    20\n",
    "16  1610612764       10         7       10        13   20    20\n",
    "17  1610612738       13         8        7        13   20    21\n",
    "18  1610612746       13        12        7         9   20    21\n",
    "19  1610612752       10        11       10        10   20    21\n",
    "20  1610612737        8         9        9        13   17    22\n",
    "21  1610612757       14        11        2        13   16    24\n",
    "22  1610612759        7        10        8        15   15    25\n",
    "23  1610612758       10        14        6        13   16    27\n",
    "24  1610612754       12        10        3        16   15    26\n",
    "25  1610612740        8        11        6        15   14    26\n",
    "26  1610612760        8        13        5        13   13    26\n",
    "27  1610612745        7        13        4        18   11    31\n",
    "28  1610612765        6        13        3        17    9    30\n",
    "29  1610612753        2        15        5        19    7    34\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_current_standings() -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> current_standings = extract_current_standings()\n",
    "    >>> type(current_standings)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> current_standings.shape\n",
    "    (30, 7)\n",
    "    >>> (current_standings['win'] == current_standings['homeWin'] + current_standings['awayWin']).sum()\n",
    "    30\n",
    "    >>> (current_standings['loss'] == current_standings['homeLoss'] + current_standings['awayLoss']).sum()\n",
    "    30\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    opt = []\n",
    "    worst_team_in_west = {}\n",
    "    worst_team_in_east = {}\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for i in package['league']['standard']['teams']:\n",
    "            if i['confRank'] == '15':\n",
    "                opt.append(i['teamId'])\n",
    "    f.close()\n",
    "    with open(file=\"teams.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for j in range(len(opt)):\n",
    "            for i in package['league']['standard']:\n",
    "                if i['teamId'] == opt[j]:\n",
    "                    if i['confName'] == 'East':\n",
    "                        worst_team_in_east['teamId'] = opt[j]\n",
    "                        worst_team_in_east['nickname'] = i['nickname']\n",
    "                        worst_team_in_east['city'] = i['city']\n",
    "                    else:\n",
    "                        worst_team_in_west['teamId'] = opt[j]\n",
    "                        worst_team_in_west['nickname'] = i['nickname']\n",
    "                        worst_team_in_west['city'] = i['city']\n",
    "\n",
    "    opt = []\n",
    "    best_team_in_west = {}\n",
    "    best_team_in_east = {}\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for i in package['league']['standard']['teams']:\n",
    "            if i['confRank'] == '1':\n",
    "                opt.append(i['teamId'])\n",
    "    f.close()\n",
    "    with open(file=\"teams.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for j in range(len(opt)):\n",
    "            for i in package['league']['standard']:\n",
    "                if i['teamId'] == opt[j]:\n",
    "                    if i['confName'] == 'East':\n",
    "                        best_team_in_east['teamId'] = opt[j]\n",
    "                        best_team_in_east['nickname'] = i['nickname']\n",
    "                        best_team_in_east['city'] = i['city']\n",
    "                    else:\n",
    "                        best_team_in_west['teamId'] = opt[j]\n",
    "                        best_team_in_west['nickname'] = i['nickname']\n",
    "                        best_team_in_west['city'] = i['city']\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "    col = [\"teamId\", \"homeWin\", \"homeLoss\", \"awayWin\", \"awayLoss\", \"win\", \"loss\"]\n",
    "    df = pd.json_normalize(package['league']['standard']['teams'])\n",
    "    df = df[col].astype(int)\n",
    "    return df[col]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Define a function `find_the_best_and_worst_team_roster()` which finds out the current team roster of the best 2 teams and the worst 2 teams based on previous 2 questions. Look into the `[\"league\"][\"standard\"]` and `[\"teamId\"]` attributes in `players.json` and `teams.json` for answer.\n",
    "\n",
    "```\n",
    "            city  nickname   teamStanding  firstName          lastName\n",
    "0        Orlando     Magic  Worst in East       Cole           Anthony\n",
    "1        Orlando     Magic  Worst in East         Mo             Bamba\n",
    "2        Orlando     Magic  Worst in East      Ignas        Brazdeikis\n",
    "3        Orlando     Magic  Worst in East    Wendell        Carter Jr.\n",
    "4        Orlando     Magic  Worst in East    Michael   Carter-Williams\n",
    "..           ...       ...            ...        ...               ...\n",
    "63  Golden State  Warriors   Best in West       Klay          Thompson\n",
    "64  Golden State  Warriors   Best in West       Juan  Toscano-Anderson\n",
    "65  Golden State  Warriors   Best in West  Quinndary      Weatherspoon\n",
    "66  Golden State  Warriors   Best in West     Andrew           Wiggins\n",
    "67  Golden State  Warriors   Best in West      James           Wiseman\n",
    "\n",
    "[68 rows x 5 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_best_and_worst_team_roster() -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> the_best_and_worst_team_roster = find_the_best_and_worst_team_roster()\n",
    "    >>> type(the_best_and_worst_team_roster)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> the_best_and_worst_team_roster.shape\n",
    "    (68, 5)\n",
    "    >>> the_best_and_worst_team_roster[\"city\"].unique()\n",
    "    array(['Orlando', 'Houston', 'Chicago', 'Golden State'], dtype=object)\n",
    "    >>> the_best_and_worst_team_roster[\"nickname\"].unique()\n",
    "    array(['Magic', 'Rockets', 'Bulls', 'Warriors'], dtype=object)\n",
    "    >>> the_best_and_worst_team_roster[\"teamStanding\"].unique()\n",
    "    array(['Worst in East', 'Worst in West', 'Best in East', 'Best in West'],\n",
    "          dtype=object)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    opt = []\n",
    "    worst_team_in_west = {}\n",
    "    worst_team_in_east = {}\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for i in package['league']['standard']['teams']:\n",
    "            if i['confRank'] == '15':\n",
    "                opt.append(i['teamId'])\n",
    "    f.close()\n",
    "    with open(file=\"teams.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for j in range(len(opt)):\n",
    "            for i in package['league']['standard']:\n",
    "                if i['teamId'] == opt[j]:\n",
    "                    if i['confName'] == 'East':\n",
    "                        worst_team_in_east['teamId'] = opt[j]\n",
    "                        worst_team_in_east['nickname'] = i['nickname']\n",
    "                        worst_team_in_east['city'] = i['city']\n",
    "                    else:\n",
    "                        worst_team_in_west['teamId'] = opt[j]\n",
    "                        worst_team_in_west['nickname'] = i['nickname']\n",
    "                        worst_team_in_west['city'] = i['city']\n",
    "\n",
    "    opt = []\n",
    "    best_team_in_west = {}\n",
    "    best_team_in_east = {}\n",
    "    with open(file=\"standings_all.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for i in package['league']['standard']['teams']:\n",
    "            if i['confRank'] == '1':\n",
    "                opt.append(i['teamId'])\n",
    "    f.close()\n",
    "    with open(file=\"teams.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "        for j in range(len(opt)):\n",
    "            for i in package['league']['standard']:\n",
    "                if i['teamId'] == opt[j]:\n",
    "                    if i['confName'] == 'East':\n",
    "                        best_team_in_east['teamId'] = opt[j]\n",
    "                        best_team_in_east['nickname'] = i['nickname']\n",
    "                        best_team_in_east['city'] = i['city']\n",
    "                    else:\n",
    "                        best_team_in_west['teamId'] = opt[j]\n",
    "                        best_team_in_west['nickname'] = i['nickname']\n",
    "                        best_team_in_west['city'] = i['city']\n",
    "\n",
    "    with open(file=\"players.json\", mode=\"r\", encoding='utf-8') as f:\n",
    "        package = json.load(f)\n",
    "    df = pd.json_normalize(package['league']['standard'])\n",
    "\n",
    "    row = df['teamId'] == \"1610612744\"\n",
    "    col = [\"firstName\", \"lastName\"]\n",
    "    df1 = df[row][col]\n",
    "    df1['teamStanding'] = 'Best in West'\n",
    "    df1['nickname'] = best_team_in_west['nickname']\n",
    "    df1['city'] = best_team_in_west['city']\n",
    "\n",
    "    row = df['teamId'] == \"1610612741\"\n",
    "    df2 = df[row][col]\n",
    "    df2['teamStanding'] = 'Best in East'\n",
    "    df2['nickname'] = best_team_in_east['nickname']\n",
    "    df2['city'] = best_team_in_east['city']\n",
    "\n",
    "    row = df['teamId'] == \"1610612745\"\n",
    "    df3 = df[row][col]\n",
    "    df3['teamStanding'] = 'Worst in West'\n",
    "    df3['nickname'] = worst_team_in_west['nickname']\n",
    "    df3['city'] = worst_team_in_west['city']\n",
    "\n",
    "    row = df['teamId'] == \"1610612753\"\n",
    "    df4 = df[row][col]\n",
    "    df4['teamStanding'] = 'Worst in East'\n",
    "    df4['nickname'] = worst_team_in_east['nickname']\n",
    "    df4['city'] = worst_team_in_east['city']\n",
    "    df = pd.concat((df1, df2, df3, df4))\n",
    "    return df\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Define a function `calculate_confirmed_death_rate_by_countries` according to the following formula according to `01-10-2022.csv` and `UID_ISO_FIPS_LookUp_Table.csv`.\n",
    "\n",
    "$$\n",
    "\\text{Death Rate} = \\frac{\\text{Deaths}}{\\text{Confirmed}} \\\\\n",
    "\\text{Confirmed Rate} = \\frac{\\text{Confirmed}}{\\text{Population}}\n",
    "$$\n",
    "\n",
    "```\n",
    "         Country_Region  Confirmed  Deaths  Population  Confirmed_Rate  \\\n",
    "0           Afghanistan     158394    7373  38928341.0        0.004069   \n",
    "1               Albania     220487    3241   2877800.0        0.076617   \n",
    "2               Algeria     222639    6349  43851043.0        0.005077   \n",
    "3               Andorra      27983     141     77265.0        0.362169   \n",
    "4                Angola      90316    1832  32866268.0        0.002748   \n",
    "..                  ...        ...     ...         ...             ...   \n",
    "191             Vietnam    1914393   34531  97338583.0        0.019667   \n",
    "192  West Bank and Gaza     472910    4987   5101416.0        0.092702   \n",
    "193               Yemen      10197    1986  29825968.0        0.000342   \n",
    "194              Zambia     284389    3817  18383956.0        0.015469   \n",
    "195            Zimbabwe     223000    5180  14862927.0        0.015004   \n",
    "\n",
    "     Death_Rate  \n",
    "0      0.046548  \n",
    "1      0.014699  \n",
    "2      0.028517  \n",
    "3      0.005039  \n",
    "4      0.020284  \n",
    "..          ...  \n",
    "191    0.018038  \n",
    "192    0.010545  \n",
    "193    0.194763  \n",
    "194    0.013422  \n",
    "195    0.023229  \n",
    "\n",
    "[196 rows x 6 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confirmed_death_rate_by_countries() -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> confirmed_death_rate_by_countries = calculate_confirmed_death_rate_by_countries()\n",
    "    >>> type(confirmed_death_rate_by_countries)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> confirmed_death_rate_by_countries.shape\n",
    "    (196, 6)\n",
    "    >>> confirmed_death_rate_by_countries[confirmed_death_rate_by_countries[\"Country_Region\"] == \"Taiwan*\"]\n",
    "        Country_Region  Confirmed  Deaths  Population  Confirmed_Rate  Death_Rate\n",
    "    172        Taiwan*      17394     850  23816775.0         0.00073    0.048867\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    df = pd.read_csv(\"01-10-2022.csv\")\n",
    "    col = ['Country_Region']\n",
    "    df_merge1 = df['Confirmed'].groupby(df['Country_Region']).sum()\n",
    "    df_merge2 = df['Deaths'].groupby(df['Country_Region']).sum()\n",
    "    df1 = pd.DataFrame(df_merge1)\n",
    "    df2 = pd.DataFrame(df_merge2)\n",
    "    df_1 = pd.merge(df1, df2, on ='Country_Region').reset_index()\n",
    "\n",
    "    df = pd.read_csv(\"UID_ISO_FIPS_LookUp_Table.csv\")\n",
    "    col = ['Country_Region', 'Population']\n",
    "    df_merge3 = df['Population'].groupby(df['Country_Region']).sum()\n",
    "    df3 = pd.DataFrame(df_merge3)\n",
    "    df_2 = pd.merge(df_1, df3, on ='Country_Region')\n",
    "    df_2['Confirmed_Rate'] = df_2['Confirmed'] / df_2['Population']\n",
    "    df_2['Death_Rate'] = df_2['Deaths'] / df_2['Confirmed']\n",
    "    return df_2\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Define a function `calculate_daily_cases()` which calculates the daily cases of a country given `time_series_covid19_confirmed_global.csv` and `time_series_covid19_deaths_global.csv`.\n",
    "\n",
    "```\n",
    "           Country/Region  Confirmed  Deaths  Daily_Confirmed  Daily_Deaths\n",
    "Date                                                                       \n",
    "2020-01-22        Taiwan*          1       0              NaN           NaN\n",
    "2020-01-23        Taiwan*          1       0              0.0           0.0\n",
    "2020-01-24        Taiwan*          3       0              2.0           0.0\n",
    "2020-01-25        Taiwan*          3       0              0.0           0.0\n",
    "2020-01-26        Taiwan*          4       0              1.0           0.0\n",
    "...                   ...        ...     ...              ...           ...\n",
    "2022-01-06        Taiwan*      17198     850             43.0           0.0\n",
    "2022-01-07        Taiwan*      17258     850             60.0           0.0\n",
    "2022-01-08        Taiwan*      17302     850             44.0           0.0\n",
    "2022-01-09        Taiwan*      17362     850             60.0           0.0\n",
    "2022-01-10        Taiwan*      17394     850             32.0           0.0\n",
    "\n",
    "[720 rows x 5 columns]\n",
    "```\n",
    "\n",
    "```\n",
    "           Country/Region  Confirmed  Deaths  Daily_Confirmed  Daily_Deaths\n",
    "Date                                                                       \n",
    "2020-01-22          Japan          2       0              NaN           NaN\n",
    "2020-01-23          Japan          2       0              0.0           0.0\n",
    "2020-01-24          Japan          2       0              0.0           0.0\n",
    "2020-01-25          Japan          2       0              0.0           0.0\n",
    "2020-01-26          Japan          4       0              2.0           0.0\n",
    "...                   ...        ...     ...              ...           ...\n",
    "2022-01-06          Japan    1741837   18395           4297.0           1.0\n",
    "2022-01-07          Japan    1747907   18396           6070.0           1.0\n",
    "2022-01-08          Japan    1756209   18398           8302.0           2.0\n",
    "2022-01-09          Japan    1764280   18399           8071.0           1.0\n",
    "2022-01-10          Japan    1770545   18401           6265.0           2.0\n",
    "\n",
    "[720 rows x 5 columns]\n",
    "```\n",
    "\n",
    "```\n",
    "           Country/Region  Confirmed  Deaths  Daily_Confirmed  Daily_Deaths\n",
    "Date                                                                       \n",
    "2020-01-22             US          1       0              NaN           NaN\n",
    "2020-01-23             US          1       0              0.0           0.0\n",
    "2020-01-24             US          2       0              1.0           0.0\n",
    "2020-01-25             US          2       0              0.0           0.0\n",
    "2020-01-26             US          5       0              3.0           0.0\n",
    "...                   ...        ...     ...              ...           ...\n",
    "2022-01-06             US   58487854  833990         786861.0        1870.0\n",
    "2022-01-07             US   59388686  836605         900832.0        2615.0\n",
    "2022-01-08             US   59767418  837266         378732.0         661.0\n",
    "2022-01-09             US   60074710  837596         307292.0         330.0\n",
    "2022-01-10             US   61558085  839500        1483375.0        1904.0\n",
    "\n",
    "[720 rows x 5 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_cases(country_name: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> tw_daily_cases = calculate_daily_cases(\"Taiwan*\")\n",
    "    >>> type(tw_daily_cases)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> tw_daily_cases.shape\n",
    "    (720, 5)\n",
    "    >>> jp_daily_cases = calculate_daily_cases(\"Japan\")\n",
    "    >>> type(jp_daily_cases)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> jp_daily_cases.shape\n",
    "    (720, 5)\n",
    "    >>> us_daily_cases = calculate_daily_cases(\"US\")\n",
    "    >>> type(us_daily_cases)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> us_daily_cases.shape\n",
    "    (720, 5)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    df1 = pd.read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "    idVars = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "    confirmed = pd.melt(df1, id_vars=idVars, var_name='Date', value_name='Confirmed')\n",
    "    row = confirmed['Country/Region'] == country_name\n",
    "    col1 = ['Country/Region', 'Confirmed', 'Date']\n",
    "    df1 = confirmed[row][col1]\n",
    "\n",
    "    df2 = pd.read_csv(\"time_series_covid19_deaths_global.csv\")\n",
    "    deaths = pd.melt(df2, id_vars=idVars, var_name='Date', value_name='Deaths')\n",
    "    col2 = ['Country/Region', 'Deaths', 'Date']\n",
    "    df2 = deaths[row][col2]\n",
    "    df = pd.merge(df1, df2, on = ['Country/Region', 'Date']).set_index('Date')\n",
    "\n",
    "    col3 = ['Confirmed', 'Deaths']\n",
    "    df_opt = df[col3]\n",
    "    df_delta = df_opt.diff(axis = 0, periods = 1)\n",
    "\n",
    "    df['Daily_Confirmed'] = df_delta['Confirmed']\n",
    "    df['Daily_Deaths'] = df_delta['Deaths']\n",
    "    return df\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Define a class `CountryCovidStatus` which instantiates objects with 2 methods `get_snapshot()` and `get_recent_two_week_trend()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountryCovidStatus:\n",
    "    \"\"\"\n",
    "    >>> tw_covid_status = CountryCovidStatus(\"Taiwan*\")\n",
    "    >>> tw_covid_status.get_snapshot()\n",
    "        Country_Region  Confirmed  Deaths  Population  Confirmed_Rate  Death_Rate\n",
    "    172        Taiwan*      17394     850  23816775.0         0.00073    0.048867\n",
    "    >>> tw_covid_status.get_recent_two_week_trend()\n",
    "               Country/Region  Confirmed  Deaths  Daily_Confirmed  Daily_Deaths\n",
    "    Date                                                                       \n",
    "    2021-12-28        Taiwan*      16950     850             19.0           0.0\n",
    "    2021-12-29        Taiwan*      16964     850             14.0           0.0\n",
    "    2021-12-30        Taiwan*      16988     850             24.0           0.0\n",
    "    2021-12-31        Taiwan*      17029     850             41.0           0.0\n",
    "    2022-01-01        Taiwan*      17050     850             21.0           0.0\n",
    "    2022-01-02        Taiwan*      17070     850             20.0           0.0\n",
    "    2022-01-03        Taiwan*      17095     850             25.0           0.0\n",
    "    2022-01-04        Taiwan*      17129     850             34.0           0.0\n",
    "    2022-01-05        Taiwan*      17155     850             26.0           0.0\n",
    "    2022-01-06        Taiwan*      17198     850             43.0           0.0\n",
    "    2022-01-07        Taiwan*      17258     850             60.0           0.0\n",
    "    2022-01-08        Taiwan*      17302     850             44.0           0.0\n",
    "    2022-01-09        Taiwan*      17362     850             60.0           0.0\n",
    "    2022-01-10        Taiwan*      17394     850             32.0           0.0\n",
    "    >>> jp_covid_status = CountryCovidStatus(\"Japan\")\n",
    "    >>> jp_covid_status.get_snapshot()\n",
    "       Country_Region  Confirmed  Deaths   Population  Confirmed_Rate  Death_Rate\n",
    "    87          Japan    1770545   18401  252643406.0        0.007008    0.010393\n",
    "    >>> jp_covid_status.get_recent_two_week_trend()\n",
    "               Country/Region  Confirmed  Deaths  Daily_Confirmed  Daily_Deaths\n",
    "    Date                                                                       \n",
    "    2021-12-28          Japan    1731048   18385            313.0           2.0\n",
    "    2021-12-29          Japan    1731446   18389            398.0           4.0\n",
    "    2021-12-30          Japan    1731878   18389            432.0           0.0\n",
    "    2021-12-31          Japan    1732296   18389            418.0           0.0\n",
    "    2022-01-01          Japan    1732752   18389            456.0           0.0\n",
    "    2022-01-02          Japan    1733229   18391            477.0           2.0\n",
    "    2022-01-03          Japan    1733901   18392            672.0           1.0\n",
    "    2022-01-04          Japan    1735050   18393           1149.0           1.0\n",
    "    2022-01-05          Japan    1737540   18394           2490.0           1.0\n",
    "    2022-01-06          Japan    1741837   18395           4297.0           1.0\n",
    "    2022-01-07          Japan    1747907   18396           6070.0           1.0\n",
    "    2022-01-08          Japan    1756209   18398           8302.0           2.0\n",
    "    2022-01-09          Japan    1764280   18399           8071.0           1.0\n",
    "    2022-01-10          Japan    1770545   18401           6265.0           2.0\n",
    "    >>> us_covid_status = CountryCovidStatus(\"US\")\n",
    "    >>> us_covid_status.get_snapshot()\n",
    "        Country_Region  Confirmed  Deaths   Population  Confirmed_Rate  Death_Rate\n",
    "    182             US   61558085  839500  994135109.0        0.061921    0.013638\n",
    "    >>> us_covid_status.get_recent_two_week_trend()\n",
    "               Country/Region  Confirmed  Deaths  Daily_Confirmed  Daily_Deaths\n",
    "    Date                                                                       \n",
    "    2021-12-28             US   53182317  821586         354683.0        2295.0\n",
    "    2021-12-29             US   53679320  823783         497003.0        2197.0\n",
    "    2021-12-30             US   54269867  825195         590547.0        1412.0\n",
    "    2021-12-31             US   54742382  825778         472515.0         583.0\n",
    "    2022-01-01             US   54858824  826045         116442.0         267.0\n",
    "    2022-01-02             US   55106998  826289         248174.0         244.0\n",
    "    2022-01-03             US   56278376  827977        1171378.0        1688.0\n",
    "    2022-01-04             US   57077603  830134         799227.0        2157.0\n",
    "    2022-01-05             US   57700993  832120         623390.0        1986.0\n",
    "    2022-01-06             US   58487854  833990         786861.0        1870.0\n",
    "    2022-01-07             US   59388686  836605         900832.0        2615.0\n",
    "    2022-01-08             US   59767418  837266         378732.0         661.0\n",
    "    2022-01-09             US   60074710  837596         307292.0         330.0\n",
    "    2022-01-10             US   61558085  839500        1483375.0        1904.0\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    def __init__(self,x):\n",
    "        self.country = x\n",
    "    \n",
    "        df = pd.read_csv(\"01-10-2022.csv\")\n",
    "        col = ['Country_Region']\n",
    "        df_merge1 = df['Confirmed'].groupby(df['Country_Region']).sum()\n",
    "        df_merge2 = df['Deaths'].groupby(df['Country_Region']).sum()\n",
    "        df1 = pd.DataFrame(df_merge1)\n",
    "        df2 = pd.DataFrame(df_merge2)\n",
    "        df_1 = pd.merge(df1, df2, on ='Country_Region').reset_index()\n",
    "\n",
    "        df = pd.read_csv(\"UID_ISO_FIPS_LookUp_Table.csv\")\n",
    "        col = ['Country_Region', 'Population']\n",
    "        df_merge3 = df['Population'].groupby(df['Country_Region']).sum()\n",
    "        df3 = pd.DataFrame(df_merge3)\n",
    "        df_2 = pd.merge(df_1, df3, on ='Country_Region')\n",
    "        df_2['Confirmed_Rate'] = df_2['Confirmed'] / df_2['Population']\n",
    "        df_2['Death_Rate'] = df_2['Deaths'] / df_2['Confirmed']\n",
    "        row2 = df_2['Country_Region'] == x\n",
    "        df_2 = df_2[row2]\n",
    "        # df_2 snapshot\n",
    "    \n",
    "        df4 = pd.read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "        idVars = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "        confirmed = pd.melt(df4, id_vars=idVars, var_name='Date', value_name='Confirmed')\n",
    "        row = confirmed['Country/Region'] == x\n",
    "        col4 = ['Country/Region', 'Confirmed', 'Date']\n",
    "        df4 = confirmed[row][col4]\n",
    "\n",
    "        df5 = pd.read_csv(\"time_series_covid19_deaths_global.csv\")\n",
    "        deaths = pd.melt(df5, id_vars=idVars, var_name='Date', value_name='Deaths')\n",
    "        col5 = ['Country/Region', 'Deaths', 'Date']\n",
    "        df5 = deaths[row][col5]\n",
    "        df = pd.merge(df4, df5, on = ['Country/Region', 'Date']).set_index('Date')\n",
    "\n",
    "        col6 = ['Confirmed', 'Deaths']\n",
    "        df_opt = df[col6]\n",
    "        df_delta = df_opt.diff(axis = 0, periods = 1)\n",
    "\n",
    "        df['Daily_Confirmed'] = df_delta['Confirmed']\n",
    "        df['Daily_Deaths'] = df_delta['Deaths']\n",
    "        # df recent\n",
    "        self.df = df\n",
    "        self.df_2 = df_2\n",
    "        \n",
    "    def get_snapshot(self):\n",
    "        return self.df_2\n",
    "    def get_recent_two_week_trend(self):\n",
    "        return self.df.tail(14)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. Define a function `import_all_sheets` which imports all 3 sheets in `imdb.xlsx` as `DataFrame` and stores in a `dict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_all_sheets() -> dict:\n",
    "    \"\"\"\n",
    "    >>> all_sheets = import_all_sheets()\n",
    "    >>> type(all_sheets)\n",
    "    dict\n",
    "    >>> len(all_sheets)\n",
    "    3\n",
    "    >>> all_sheets[\"movies\"].shape\n",
    "    (250, 6)\n",
    "    >>> all_sheets[\"casting\"].shape\n",
    "    (3584, 3)\n",
    "    >>> all_sheets[\"actors\"].shape\n",
    "    (3108, 2)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    ans = {}\n",
    "    xls = pd.ExcelFile(\"imdb.xlsx\")\n",
    "    for i in range(len(xls.sheet_names)):\n",
    "        ans[xls.sheet_names[i]] = pd.read_excel(\"imdb.xlsx\", sheet_name = i)\n",
    "    return ans\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Define a function `find_movie_by_actor_name` which finds movies by actor based on `imdb.xlsx`. You may refer to the relationship between the three sheets by viewing their entity relationship.\n",
    "\n",
    "![](imdb_erd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_movie_by_actor_name(*args) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> find_movie_by_actor_name(\"Tom Hanks\")\n",
    "            id                title  release_year  rating          director  \\\n",
    "    11    12.0         Forrest Gump        1994.0     8.8   Robert Zemeckis   \n",
    "    25    26.0  Saving Private Ryan        1998.0     8.6  Steven Spielberg   \n",
    "    26    27.0       The Green Mile        1999.0     8.6    Frank Darabont   \n",
    "    81    82.0            Toy Story        1995.0     8.3     John Lasseter   \n",
    "    111  112.0          Toy Story 3        2010.0     8.2       Lee Unkrich   \n",
    "    188  189.0  Catch Me If You Can        2002.0     8.1  Steven Spielberg   \n",
    "\n",
    "         runtime  \n",
    "    11     142.0  \n",
    "    25     169.0  \n",
    "    26     189.0  \n",
    "    81      81.0  \n",
    "    111    103.0  \n",
    "    188    141.0 \n",
    "    >>> find_movie_by_actor_name(\"Leonardo DiCaprio\")\n",
    "            id                    title  release_year  rating           director  \\\n",
    "    12    13.0                Inception        2010.0     8.8  Christopher Nolan   \n",
    "    41    42.0             The Departed        2006.0     8.5    Martin Scorsese   \n",
    "    56    57.0         Django Unchained        2012.0     8.4  Quentin Tarantino   \n",
    "    139  140.0  The Wolf of Wall Street        2013.0     8.2    Martin Scorsese   \n",
    "    150  151.0           Shutter Island        2010.0     8.2    Martin Scorsese   \n",
    "    188  189.0      Catch Me If You Can        2002.0     8.1   Steven Spielberg   \n",
    "\n",
    "         runtime  \n",
    "    12     148.0  \n",
    "    41     151.0  \n",
    "    56     165.0  \n",
    "    139    180.0  \n",
    "    150    138.0  \n",
    "    188    141.0\n",
    "    >>> find_movie_by_actor_name(\"Tom Hanks\", \"Leonardo DiCaprio\")\n",
    "            id                    title  release_year  rating           director  \\\n",
    "    11    12.0             Forrest Gump        1994.0     8.8    Robert Zemeckis   \n",
    "    12    13.0                Inception        2010.0     8.8  Christopher Nolan   \n",
    "    25    26.0      Saving Private Ryan        1998.0     8.6   Steven Spielberg   \n",
    "    26    27.0           The Green Mile        1999.0     8.6     Frank Darabont   \n",
    "    41    42.0             The Departed        2006.0     8.5    Martin Scorsese   \n",
    "    56    57.0         Django Unchained        2012.0     8.4  Quentin Tarantino   \n",
    "    81    82.0                Toy Story        1995.0     8.3      John Lasseter   \n",
    "    111  112.0              Toy Story 3        2010.0     8.2        Lee Unkrich   \n",
    "    139  140.0  The Wolf of Wall Street        2013.0     8.2    Martin Scorsese   \n",
    "    150  151.0           Shutter Island        2010.0     8.2    Martin Scorsese   \n",
    "    188  189.0      Catch Me If You Can        2002.0     8.1   Steven Spielberg   \n",
    "\n",
    "         runtime  \n",
    "    11     142.0  \n",
    "    12     148.0  \n",
    "    25     169.0  \n",
    "    26     189.0  \n",
    "    41     151.0  \n",
    "    56     165.0  \n",
    "    81      81.0  \n",
    "    111    103.0  \n",
    "    139    180.0  \n",
    "    150    138.0  \n",
    "    188    141.0\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    ans = {}\n",
    "    xls = pd.ExcelFile(\"imdb.xlsx\")\n",
    "    for i in range(len(xls.sheet_names)):\n",
    "        ans[xls.sheet_names[i]] = pd.read_excel(\"imdb.xlsx\", sheet_name = i)\n",
    "\n",
    "    movies = ans[xls.sheet_names[0]]\n",
    "    casting = ans[xls.sheet_names[1]]\n",
    "    actors = ans[xls.sheet_names[2]]\n",
    "    \n",
    "    opt = []  # 要找的電影編號\n",
    "    for i in range(len(args)):\n",
    "        actor = args[i]\n",
    "        filt1 = actors['name'] == actor\n",
    "        series1 = actors[filt1]['id']\n",
    "        for value in series1.keys():\n",
    "            id_ = float(series1[value])\n",
    "\n",
    "        filt2 = casting['actor_id'] == id_\n",
    "        series2 = casting[filt2]\n",
    "        #print(series2)\n",
    "\n",
    "        for j in series2['movie_id'].keys():\n",
    "            opt.append(float(series2['movie_id'][j]))\n",
    "    \n",
    "    filt3 = movies['id'].isin(opt) \n",
    "    return movies[filt3]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define a function `create_trilogy_dataframe` which integrates the information provided by `imdb.xlsx` and creates a `DataFrame` of famous trilogies \"The Lord of the Rings\" directed by Peter Jackson and \"The Dark Knight\" directed by Christopher Nolan. You may refer to the relationship between the three sheets by viewing their entity relationship.\n",
    "\n",
    "![](imdb_erd.png)\n",
    "\n",
    "```\n",
    "                                    title           director  \\\n",
    "0                           Batman Begins  Christopher Nolan   \n",
    "1                           Batman Begins  Christopher Nolan   \n",
    "2                           Batman Begins  Christopher Nolan   \n",
    "3                           Batman Begins  Christopher Nolan   \n",
    "4                           Batman Begins  Christopher Nolan   \n",
    "..                                    ...                ...   \n",
    "85  The Lord of the Rings: The Two Towers      Peter Jackson   \n",
    "86  The Lord of the Rings: The Two Towers      Peter Jackson   \n",
    "87  The Lord of the Rings: The Two Towers      Peter Jackson   \n",
    "88  The Lord of the Rings: The Two Towers      Peter Jackson   \n",
    "89  The Lord of the Rings: The Two Towers      Peter Jackson   \n",
    "\n",
    "                actor  \n",
    "0      Christian Bale  \n",
    "1       Michael Caine  \n",
    "2         Liam Neeson  \n",
    "3        Katie Holmes  \n",
    "4         Gary Oldman  \n",
    "..                ...  \n",
    "85      Calum Gittins  \n",
    "86       Bernard Hill  \n",
    "87      Bruce Hopkins  \n",
    "88  Paris Howe Strewe  \n",
    "89    Christopher Lee  \n",
    "\n",
    "[90 rows x 3 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trilogy_dataframe() -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> trilogy_dataframe = create_trilogy_dataframe()\n",
    "    >>> type(trilogy_dataframe)\n",
    "    pandas.core.frame.DataFrame\n",
    "    >>> trilogy_dataframe.shape\n",
    "    (90, 3)\n",
    "    >>> trilogy_dataframe[\"title\"].nunique()\n",
    "    6\n",
    "    >>> trilogy_dataframe[\"director\"].nunique()\n",
    "    2\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    ans = {}\n",
    "    xls = pd.ExcelFile(\"imdb.xlsx\")\n",
    "    for i in range(len(xls.sheet_names)):\n",
    "        ans[xls.sheet_names[i]] = pd.read_excel(\"imdb.xlsx\", sheet_name = i)\n",
    "\n",
    "    movies = ans[xls.sheet_names[0]]\n",
    "    casting = ans[xls.sheet_names[1]]\n",
    "    actors = ans[xls.sheet_names[2]]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    opt = ['Batman Begins', 'The Dark Knight', 'The Dark Knight Rises', \n",
    "           'The Lord of the Rings: The Return of the King', 'The Lord of the Rings: The Fellowship of the Ring', \n",
    "           'The Lord of the Rings: The Two Towers']  # 要找的電影\n",
    "    opt = [val for val in opt for i in range(15)]\n",
    "\n",
    "    filt1 = movies['title'].isin(opt)\n",
    "\n",
    "    nums = []\n",
    "    series1 = movies[filt1]['id']\n",
    "    for value in series1.keys():\n",
    "        id_ = float(series1[value])\n",
    "        nums.append(id_)\n",
    "\n",
    "    director = []\n",
    "    series3 = movies[filt1]['director']\n",
    "    for value in series3.keys():\n",
    "        director.append(series3[value])\n",
    "    director = [val for val in director for i in range(15)]\n",
    "\n",
    "    df['title'] = opt\n",
    "    df['director'] = director\n",
    "\n",
    "    cast = []\n",
    "    for i in range(len(nums)):\n",
    "        filt2 = casting['movie_id'] == nums[i]\n",
    "        series2 = casting[filt2] # 所有候選電影的卡司\n",
    "        for j in series2['actor_id'].keys():\n",
    "            cast.append(float(series2['actor_id'][j]))\n",
    "\n",
    "    actor = []\n",
    "    for j in range(len(cast)):\n",
    "        filt4 = actors['id'] == cast[j]\n",
    "        for k in actors[filt4]['name'].keys():\n",
    "                actor.append(actors[filt4]['name'][k])\n",
    "\n",
    "    df['actor'] = actor\n",
    "    return df\n",
    "    ### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
